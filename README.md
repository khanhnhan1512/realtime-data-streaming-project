<h1 align="center">Realtime Data Streaming Pipeline</h1><h1 align="center">Realtime Data Streaming Pipeline</h1>



<p align="center">ÄÃ¢y lÃ  dá»± Ã¡n cÃ¡ nhÃ¢n xÃ¢y dá»±ng má»™t pipeline realtime streaming sá»­ dá»¥ng `Apache Kafka`, `Apache Spark Streaming` vÃ  `Apache Airflow`. Má»¥c tiÃªu cá»§a pipeline lÃ  thu tháº­p dá»¯ liá»‡u ngÆ°á»i dÃ¹ng tá»« má»™t API giáº£ láº­p, xá»­ lÃ½ dá»¯ liá»‡u theo thá»i gian vÃ  lÆ°u trá»¯ chÃºng vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u `Cassandra`.

  <a href="README.md">English</a> Â·

  <a href="README.vi.md">Tiáº¿ng Viá»‡t</a>KhÃ´ng cÃ³ viá»‡c xá»­ lÃ½ dá»¯ liá»‡u nÃ o quÃ¡ phá»©c táº¡p trong pipeline nÃ y, má»¥c Ä‘Ã­ch cá»§a project nÃ y chá»‰ lÃ  Ä‘á»ƒ lÃ m quen vá»›i viá»‡c sá»­ dá»¥ng cÃ¡c cÃ´ng cá»¥ phá»• biáº¿n trong lÄ©nh vá»±c xá»­ lÃ½ dá»¯ liá»‡u lá»›n vÃ  realtime streaming.

</p>

# Má»¥c lá»¥c

This is a personal project building a realtime streaming pipeline using `Apache Kafka`, `Apache Spark Streaming`, and `Apache Airflow`. The pipeline's goal is to collect user data from a mock API, process data in real-time, and store it in a `Cassandra` database.- [Tá»•ng quan](#tá»•ng-quan)

- [Kiáº¿n trÃºc pipeline](#kiáº¿n-trÃºc-pipeline)

There is no overly complex data processing in this pipeline; the project's purpose is simply to familiarize myself with common tools in the field of big data processing and realtime streaming.- [Cáº¥u trÃºc thÆ° má»¥c](#cáº¥u-trÃºc-thÆ°-má»¥c)

- [Luá»“ng dá»¯ liá»‡u](#luá»“ng-dá»¯-liá»‡u)

# Table of Contents- [CÃ´ng nghá»‡ sá»­ dá»¥ng](#cÃ´ng-nghá»‡-sá»­-dá»¥ng)

- [Overview](#overview)- [YÃªu cáº§u cÃ i Ä‘áº·t](#yÃªu-cáº§u-cÃ i-Ä‘áº·t)

- [Pipeline Architecture](#pipeline-architecture)- [HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  khá»Ÿi cháº¡y](#hÆ°á»›ng-dáº«n-cÃ i-Ä‘áº·t-vÃ -khá»Ÿi-cháº¡y)

- [Project Structure](#project-structure)- [GiÃ¡m sÃ¡t vÃ  quáº£n lÃ½](#giÃ¡m-sÃ¡t-vÃ -quáº£n-lÃ½)

- [Data Flow](#data-flow)

- [Technologies Used](#technologies-used)# Tá»•ng quan

- [Installation Requirements](#installation-requirements)

- [Installation and Launch Guide](#installation-and-launch-guide)Pipeline nÃ y Ä‘Æ°á»£c xÃ¢y dá»±ng nháº±m má»¥c Ä‘Ã­ch há»c táº­p vÃ  lÃ m quen vá»›i cÃ¡c cÃ´ng cá»¥ xá»­ lÃ½ dá»¯ liá»‡u lá»›n vÃ  realtime streaming. Dá»¯ liá»‡u ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c giáº£ láº­p tá»« má»™t API Ä‘Æ¡n giáº£n, sau Ä‘Ã³ Ä‘Æ°á»£c gá»­i vÃ o Kafka Ä‘á»ƒ xá»­ lÃ½ theo thá»i gian thá»±c báº±ng Spark Streaming. Cuá»‘i cÃ¹ng, dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ sáº½ Ä‘Æ°á»£c lÆ°u trá»¯ vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Cassandra Ä‘á»ƒ phá»¥c vá»¥ cho cÃ¡c má»¥c Ä‘Ã­ch phÃ¢n tÃ­ch sau nÃ y.

- [Monitoring and Management](#monitoring-and-management)

# Kiáº¿n trÃºc pipeline

# Overview![Pipeline Architecture](./images/pipeline-architecture.svg)



This pipeline is built for learning purposes and to familiarize myself with big data processing and realtime streaming tools. User data is simulated from a simple API, then sent to Kafka for real-time processing using Spark Streaming. Finally, the processed data is stored in a Cassandra database for later analysis purposes.# Cáº¥u trÃºc thÆ° má»¥c

```

# Pipeline Architectureâ”œâ”€â”€ ğŸ“ airflow

![Pipeline Architecture](./images/pipeline-architecture.svg)â”‚   â”œâ”€â”€ ğŸ“ dags                             # chá»©a cÃ¡c DAGs cho Apache Airflow

â”‚   â”‚   â””â”€â”€ ğŸ kafka_stream.py

# Project Structureâ”‚   â””â”€â”€ ğŸ³ Dockerfile                       # Dockerfile Ä‘á»ƒ xÃ¢y dá»±ng image Airflow 

```â”œâ”€â”€ ğŸ“ api-request                          

â”œâ”€â”€ ğŸ“ airflowâ”‚   â”œâ”€â”€ ğŸ“ src      # ÄÃ³ng gÃ³i `user_data_api` thÃ nh 1 package Ä‘á»ƒ cháº¡y trÃªn mÃ´i trÆ°á»ng local

â”‚   â”œâ”€â”€ ğŸ“ dags                             # contains DAGs for Apache Airflowâ”‚   â”‚   â”œâ”€â”€ ğŸ“ api_request.egg-info         

â”‚   â”‚   â””â”€â”€ ğŸ kafka_stream.pyâ”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ PKG-INFO

â”‚   â””â”€â”€ ğŸ³ Dockerfile                       # Dockerfile to build Airflow image â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ SOURCES.txt

â”œâ”€â”€ ğŸ“ api-request                          â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dependency_links.txt

â”‚   â”œâ”€â”€ ğŸ“ src      # Package `user_data_api` as a package to run on local environmentâ”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requires.txt

â”‚   â”‚   â”œâ”€â”€ ğŸ“ api_request.egg-info         â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ top_level.txt

â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ PKG-INFOâ”‚   â”‚   â”œâ”€â”€ ğŸ __init__.py

â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ SOURCES.txtâ”‚   â”‚   â””â”€â”€ ğŸ user_data_api.py             # Script láº¥y data tá»« API

â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dependency_links.txtâ”‚   â”œâ”€â”€ ğŸ“ README.md

â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requires.txtâ”‚   â”œâ”€â”€ ğŸ __init__.py

â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ top_level.txtâ”‚   â”œâ”€â”€ ğŸ main.py

â”‚   â”‚   â”œâ”€â”€ ğŸ __init__.pyâ”‚   â””â”€â”€ âš™ï¸ pyproject.toml

â”‚   â”‚   â””â”€â”€ ğŸ user_data_api.py             # Script to fetch data from APIâ”œâ”€â”€ ğŸ“ config

â”‚   â”œâ”€â”€ ğŸ“ README.mdâ”‚   â””â”€â”€ ğŸ“„ airflow.cfg

â”‚   â”œâ”€â”€ ğŸ __init__.pyâ”œâ”€â”€ ğŸ“ images                               # Chá»©a cÃ¡c file áº£nh

â”‚   â”œâ”€â”€ ğŸ main.pyâ”‚   â”œâ”€â”€ ğŸ“„ pipeline-architecture.drawio

â”‚   â””â”€â”€ âš™ï¸ pyproject.tomlâ”‚   â”œâ”€â”€ ğŸ–¼ï¸ pipeline-architecture.png

â”œâ”€â”€ ğŸ“ configâ”‚   â””â”€â”€ ğŸ–¼ï¸ pipeline-architecture.svg

â”‚   â””â”€â”€ ğŸ“„ airflow.cfgâ”œâ”€â”€ ğŸ“ script                  # Chá»©a entrypoint cho cÃ¡c service

â”œâ”€â”€ ğŸ“ images                               # Contains image filesâ”œâ”€â”€ ğŸ“ spark

â”‚   â”œâ”€â”€ ğŸ“„ pipeline-architecture.drawioâ”‚   â”œâ”€â”€ ğŸ³ Dockerfile          # Dockerfile Ä‘á»ƒ xÃ¢y dá»±ng custom image Spark 

â”‚   â”œâ”€â”€ ğŸ–¼ï¸ pipeline-architecture.pngâ”‚   â””â”€â”€ ğŸ spark_stream.py     # Script cháº¡y Spark job

â”‚   â””â”€â”€ ğŸ–¼ï¸ pipeline-architecture.svgâ”œâ”€â”€ âš™ï¸ .dockerignore

â”œâ”€â”€ ğŸ“ script                  # Contains entrypoints for servicesâ”œâ”€â”€ âš™ï¸ .gitignore

â”œâ”€â”€ ğŸ“ sparkâ”œâ”€â”€ ğŸ“ README.md

â”‚   â”œâ”€â”€ ğŸ³ Dockerfile          # Dockerfile to build custom Spark image â”œâ”€â”€ âš™ï¸ docker-compose.airflow3.yaml # Docker Compose cho Apache Airflow

â”‚   â””â”€â”€ ğŸ spark_stream.py     # Script to run Spark jobâ”œâ”€â”€ âš™ï¸ docker-compose.kafka.yaml    # Docker Compose cho Apache Kafka

â”œâ”€â”€ âš™ï¸ .dockerignoreâ”œâ”€â”€ âš™ï¸ docker-compose.spark.yaml    # Docker Compose cho Apache Spark

â”œâ”€â”€ âš™ï¸ .gitignoreâ”œâ”€â”€ âš™ï¸ pyproject.toml               # File cáº¥u hÃ¬nh quáº£n lÃ½ mÃ´i trÆ°á»ng áº£o báº±ng uv

â”œâ”€â”€ ğŸ“ README.mdâ”œâ”€â”€ ğŸ“„ requirements.txt             # File chá»©a cÃ¡c dependencies cá»§a mÃ´i trÆ°á»ng áº£o

â”œâ”€â”€ âš™ï¸ docker-compose.airflow3.yaml # Docker Compose for Apache Airflowâ”œâ”€â”€ ğŸ“„ stop-pipeline.sh             # Shell script Ä‘á»ƒ dá»«ng toÃ n bá»™ pipeline

â”œâ”€â”€ âš™ï¸ docker-compose.kafka.yaml    # Docker Compose for Apache Kafkaâ””â”€â”€ ğŸ“„ uv.lock                      # File khÃ³a mÃ´i trÆ°á»ng áº£o cá»§a uv

â”œâ”€â”€ âš™ï¸ docker-compose.spark.yaml    # Docker Compose for Apache Spark```

â”œâ”€â”€ âš™ï¸ pyproject.toml               # Configuration file for managing virtual environment with uv# Luá»“ng dá»¯ liá»‡u

â”œâ”€â”€ ğŸ“„ requirements.txt             # File containing virtual environment dependencies1. Dá»¯ liá»‡u giáº£ láº­p vá» ngÆ°á»i dÃ¹ng Ä‘Æ°á»£c láº¥y tá»« [API](https://randomuser.me/api). API nÃ y cung cáº¥p dá»¯ liá»‡u ngÆ°á»i dÃ¹ng ngáº«u nhiÃªn vá»›i cÃ¡c thÃ´ng tin nhÆ° tÃªn, Ä‘á»‹a chá»‰ email, quá»‘c gia, v.v.

â”œâ”€â”€ ğŸ“„ stop-pipeline.sh             # Shell script to stop the entire pipeline2. Dá»¯ liá»‡u cá»§a tá»«ng ngÆ°á»i dÃ¹ng (record) Ä‘Æ°á»£c gá»­i Ä‘áº¿n má»™t chá»§ Ä‘á» (topic) trong Apache Kafka thÃ´ng qua má»™t producer.

â””â”€â”€ ğŸ“„ uv.lock                      # Virtual environment lock file for uv3. Apache Airflow Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u phá»‘i 2 quÃ¡ trÃ¬nh trÃªn.

```4. Apache Spark Streaming Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘á»ƒ láº¯ng nghe chá»§ Ä‘á» Kafka vÃ  nháº­n dá»¯ liá»‡u theo thá»i gian thá»±c.

5. Dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c tá»« Kafka sáº½ Ä‘Æ°á»£c Spark nháº­n, truyá»n Ä‘i vÃ  lÆ°u trá»¯ vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Cassandra.

# Data Flow

1. Simulated user data is retrieved from the [API](https://randomuser.me/api). This API provides random user data with information such as name, email address, country, etc.# CÃ´ng nghá»‡ sá»­ dá»¥ng

2. Each user's data (record) is sent to a topic in Apache Kafka through a producer.| CÃ´ng nghá»‡        | Chá»©c nÄƒng                                                                                      |

3. Apache Airflow is used to orchestrate the above 2 processes.|------------------|------------------------------------------------------------------------------------------------|

4. Apache Spark Streaming is configured to listen to the Kafka topic and receive data in real-time.| Docker           | ÄÃ³ng gÃ³i vÃ  cháº¡y toÃ n bá»™ cÃ¡c service                                                           |

5. Data received from Kafka will be received by Spark, transmitted, and stored in the Cassandra database.| Apache Kafka     | Tiáº¿p nháº­n vÃ  lÆ°u trá»¯ cÃ¡c record vá»›i lÆ°u lÆ°á»£ng cao                                              |

| Zookeeper        | Quáº£n lÃ½, Ä‘iá»u phá»‘i Kafka Cluster                                                               |

# Technologies Used| Schema Registry  | Quáº£n lÃ½ vÃ  xÃ¡c thá»±c schema cho cÃ¡c record gá»­i tá»›i Kafka                                        |

| Technology       | Function                                                                                       || Control Center   | Giao diá»‡n Ä‘á»ƒ quáº£n lÃ½ vÃ  giÃ¡m sÃ¡t Kafka Cluster                                                 |

|------------------|------------------------------------------------------------------------------------------------|| Spark Streaming  | Láº¯ng nghe 1 topic tá»« Kafka vÃ  truyá»n dá»¯ liá»‡u má»›i trong topic Ä‘áº¿n Cassandra theo thá»i gian thá»±c |

| Docker           | Package and run all services                                                                   || Apache Cassandra | CÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»ƒ lÆ°u trá»¯ record thá»i gian thá»±c                                                 |

| Apache Kafka     | Receive and store high-volume records                                                          || Apache Airflow   | GiÃ¡m sÃ¡t vÃ  Ä‘iá»u phá»‘i cÃ¡c cÃ´ng viá»‡c trong pipeline                                             |

| Zookeeper        | Manage and coordinate Kafka Cluster                                                            || PostgresSQL      | LÆ°u trá»¯ metadata cho Airflow                                                                   |

| Schema Registry  | Manage and validate schema for records sent to Kafka                                           |# YÃªu cáº§u cÃ i Ä‘áº·t

| Control Center   | Interface to manage and monitor Kafka Cluster                                                  |Äá»ƒ cháº¡y Ä‘Æ°á»£c project nÃ y cáº§n cÃ¡c Ä‘iá»u kiá»‡n sau:

| Spark Streaming  | Listen to a topic from Kafka and transmit new data in the topic to Cassandra in real-time     |- CÃ i Ä‘áº·t Docker vÃ  Docker Compose.

| Apache Cassandra | Database to store real-time records                                                            |- Python 3.11

| Apache Airflow   | Monitor and orchestrate jobs in the pipeline                                                   |- MÃ´i trÆ°á»ng áº£o cÃ³ cÃ¡c thÆ° viá»‡n nhÆ° trong `requirements.txt`.

| PostgreSQL       | Store metadata for Airflow                                                                     |- File `.env` chá»©a cÃ¡c biáº¿n mÃ´i trÆ°á»ng Ä‘á»ƒ cháº¡y Ä‘Æ°á»£c cÃ¡c service cá»§a `Airflow` nhÆ°:

    - `AIRFLOW_UID`

# Installation Requirements    - `AIRFLOW_GID`

To run this project, the following conditions are required:    - `AIRFLOW_PROJ_DIR`

- Docker and Docker Compose installed.    - `_AIRFLOW_WWW_USER_USERNAME`

- Python 3.11    - `_AIRFLOW_WWW_USER_PASSWORD`

- Virtual environment with libraries as in `requirements.txt`.# HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  khá»Ÿi cháº¡y

- `.env` file containing environment variables to run `Airflow` services such as:1. Clone repository nÃ y vá» mÃ¡y cá»§a báº¡n:

    - `AIRFLOW_UID`    ```bash

    - `AIRFLOW_GID`    git clone https://github.com/khanhnhan1512/realtime-data-streaming-project.git

    - `AIRFLOW_PROJ_DIR`    ```

    - `_AIRFLOW_WWW_USER_USERNAME`2. Di chuyá»ƒn vÃ o thÆ° má»¥c dá»± Ã¡n:

    - `_AIRFLOW_WWW_USER_PASSWORD`    ```bash

    cd realtime-data-streaming-project

# Installation and Launch Guide    ```

1. Clone this repository to your machine:3. Táº¡o vÃ  kÃ­ch hoáº¡t mÃ´i trÆ°á»ng áº£o:

    ```bash    - Náº¿u sá»­ dá»¥ng `uv`:

    git clone https://github.com/khanhnhan1512/realtime-data-streaming-project.git        ```bash

    ```        uv init

2. Navigate to the project directory:        uv sync

    ```bash        ```

    cd realtime-data-streaming-project    - Náº¿u sá»­ dá»¥ng `venv`:

    ```        ```bash

3. Create and activate virtual environment:        python3 -m venv venv

    - If using `uv`:        source venv/bin/activate

        ```bash        pip install -r requirements.txt

        uv init        ```

        uv sync4. Cháº¡y Docker Compose Ä‘á»ƒ khá»Ÿi Ä‘á»™ng toÃ n bá»™ pipeline:

        ```    ```bash

    - If using `venv`:    docker-compose -f docker-compose.kafka.yaml up -d # Ä‘á»£i khoáº£ng 20s Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng xong

        ```bash    docker-compose -f docker-compose.airflow3.yaml up -d  # Ä‘á»£i khoáº£ng 20s Ä‘á»ƒ khá»Ÿi Ä‘á»™ng Airflow

        python3 -m venv venv    docker-compose -f docker-compose.spark.yaml up -d  # khá»Ÿi Ä‘á»™ng Spark

        source venv/bin/activate    ```

        pip install -r requirements.txt5. Dá»«ng toÃ n bá»™ pipeline:

        ```    ```bash

4. Run Docker Compose to start the entire pipeline:    ./stop-pipeline.sh

    ```bash    ```

    docker-compose -f docker-compose.kafka.yaml up -d # wait about 20s for Kafka to start# GiÃ¡m sÃ¡t vÃ  quáº£n lÃ½

    docker-compose -f docker-compose.airflow3.yaml up -d  # wait about 20s to start Airflow- Control Center cá»§a Kafka: [http://localhost:9021](http://localhost:9021)

    docker-compose -f docker-compose.spark.yaml up -d  # start Spark- Giao diá»‡n web cá»§a Airflow: [http://localhost:8081](http://localhost:8081) (username vÃ  password Ä‘Æ°á»£c cáº¥u hÃ¬nh trong file `.env`, máº·c Ä‘á»‹nh lÃ  admin/admin)

    ```- Giao diá»‡n web cá»§a Spark: [http://localhost:9090](http://localhost:9090)
5. Stop the entire pipeline:
    ```bash
    ./stop-pipeline.sh
    ```

# Monitoring and Management
- Kafka Control Center: [http://localhost:9021](http://localhost:9021)
- Airflow web interface: [http://localhost:8081](http://localhost:8081) (username and password are configured in the `.env` file, default is admin/admin)
- Spark web interface: [http://localhost:9090](http://localhost:9090)
