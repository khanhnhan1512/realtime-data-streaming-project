<h1 align="center">Realtime Data Streaming Pipeline</h1>

<p align="center">
  <a href="README.md">English</a> Â·
  <a href="README.vi.md">Tiáº¿ng Viá»‡t</a>
</p>

This is a personal project to build a real-time streaming pipeline using `Apache Kafka`, `Apache Spark Streaming`, and `Apache Airflow`. The goal of this pipeline is to collect user data from a mock API, process the data in real-time, and store it in a `Cassandra` database.

There is no complex data processing in this pipeline; the purpose of this project is simply to become familiar with using popular tools in the field of big data processing and real-time streaming.

# Table of Contents
- [Overview](#overview)
- [Pipeline Architecture](#pipeline-architecture)
- [Folder Structure](#folder-structure)
- [Data Flow](#data-flow)
- [Technologies Used](#technologies-used)
- [Installation Requirements](#installation-requirements)
- [Installation and Setup Guide](#installation-and-setup-guide)
- [Monitoring and Management](#monitoring-and-management)

# Overview

This pipeline is built for learning purposes, aiming to familiarize with big data processing and real-time streaming tools. User data is simulated from a simple API, then sent to Kafka for real-time processing using Spark Streaming. Finally, the processed data is stored in a Cassandra database to serve future analytical purposes.

# Pipeline Architecture
![Pipeline Architecture](./images/pipeline-architecture.svg)

# Folder Structure
```
â”œâ”€â”€ ğŸ“ airflow
â”‚   â”œâ”€â”€ ğŸ“ dags                             # Contains DAGs for Apache Airflow
â”‚   â”‚   â””â”€â”€ ğŸ kafka_stream.py
â”‚   â””â”€â”€ ğŸ³ Dockerfile                       # Dockerfile to build Airflow image
â”œâ”€â”€ ğŸ“ api-request                          
â”‚   â”œâ”€â”€ ğŸ“ src      # Package `user_data_api` as a package to run in local environment
â”‚   â”‚   â”œâ”€â”€ ğŸ“ api_request.egg-info         
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ PKG-INFO
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ SOURCES.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dependency_links.txt
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ requires.txt
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ top_level.txt
â”‚   â”‚   â”œâ”€â”€ ğŸ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ user_data_api.py             # Script to fetch data from API
â”‚   â”œâ”€â”€ ğŸ“ README.md
â”‚   â”œâ”€â”€ ğŸ __init__.py
â”‚   â”œâ”€â”€ ğŸ main.py
â”‚   â””â”€â”€ âš™ï¸ pyproject.toml
â”œâ”€â”€ ğŸ“ config
â”‚   â””â”€â”€ ğŸ“„ airflow.cfg
â”œâ”€â”€ ğŸ“ images                               # Contains image files
â”‚   â”œâ”€â”€ ğŸ“„ pipeline-architecture.drawio
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ pipeline-architecture.png
â”‚   â””â”€â”€ ğŸ–¼ï¸ pipeline-architecture.svg
â”œâ”€â”€ ğŸ“ script                  # Contains entrypoints for services
â”œâ”€â”€ ğŸ“ spark
â”‚   â”œâ”€â”€ ğŸ³ Dockerfile          # Dockerfile to build custom Spark image
â”‚   â””â”€â”€ ğŸ spark_stream.py     # Script to run Spark job
â”œâ”€â”€ âš™ï¸ .dockerignore
â”œâ”€â”€ âš™ï¸ .gitignore
â”œâ”€â”€ ğŸ“ README.md
â”œâ”€â”€ âš™ï¸ docker-compose.airflow3.yaml # Docker Compose for Apache Airflow
â”œâ”€â”€ âš™ï¸ docker-compose.kafka.yaml    # Docker Compose for Apache Kafka
â”œâ”€â”€ âš™ï¸ docker-compose.spark.yaml    # Docker Compose for Apache Spark
â”œâ”€â”€ âš™ï¸ pyproject.toml               # Configuration file for virtual environment management using uv
â”œâ”€â”€ ğŸ“„ requirements.txt             # File containing virtual environment dependencies
â”œâ”€â”€ ğŸ“„ stop-pipeline.sh             # Shell script to stop the entire pipeline
â””â”€â”€ ğŸ“„ uv.lock                      # Virtual environment lock file for uv
```

# Data Flow
1. Simulated user data is fetched from the [API](https://randomuser.me/api). This API provides random user data with information such as name, email address, country, etc.
2. Each user's data (record) is sent to a topic in Apache Kafka through a producer.
3. Apache Airflow is used to orchestrate the above 2 processes.
4. Apache Spark Streaming is configured to listen to the Kafka topic and receive data in real-time.
5. Data received from Kafka will be processed by Spark, streamed, and stored in the Cassandra database.

# Technologies Used
| Technology       | Function                                                                                    |
|------------------|---------------------------------------------------------------------------------------------|
| Docker           | Package and run all services                                                                |
| Apache Kafka     | Receive and store high-throughput records                                                   |
| Zookeeper        | Manage and coordinate Kafka Cluster                                                         |
| Schema Registry  | Manage and validate schemas for records sent to Kafka                                       |
| Control Center   | Interface to manage and monitor Kafka Cluster                                               |
| Spark Streaming  | Listen to a topic from Kafka and stream new data in the topic to Cassandra in real-time    |
| Apache Cassandra | Database to store real-time records                                                         |
| Apache Airflow   | Monitor and orchestrate tasks in the pipeline                                               |
| PostgreSQL       | Store metadata for Airflow                                                                  |

# Prerequisites
To run this project, you need:
- Docker and Docker Compose installed.
- Python 3.11
- Virtual environment with libraries as specified in `requirements.txt`.
- `.env` file containing environment variables to run `Airflow` services such as:
    - `AIRFLOW_UID`
    - `AIRFLOW_GID`
    - `AIRFLOW_PROJ_DIR`
    - `_AIRFLOW_WWW_USER_USERNAME`
    - `_AIRFLOW_WWW_USER_PASSWORD`

# Installation and Setup Guide
1. Clone this repository to your machine:
    ```bash
    git clone https://github.com/khanhnhan1512/realtime-data-streaming-project.git
    ```
2. Navigate to the project directory:
    ```bash
    cd realtime-data-streaming-project
    ```
3. Create and activate virtual environment:
    - If using `uv`:
        ```bash
        uv init
        uv sync
        ```
    - If using `venv`:
        ```bash
        python3 -m venv venv
        source venv/bin/activate
        pip install -r requirements.txt
        ```
4. Run Docker Compose to start the entire pipeline:
    ```bash
    docker-compose -f docker-compose.kafka.yaml up -d # wait about 20s for Kafka to start
    docker-compose -f docker-compose.airflow3.yaml up -d  # wait about 20s to start Airflow
    docker-compose -f docker-compose.spark.yaml up -d  # start Spark
    ```
5. Submit Spark job:
    ```bash
    docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --conf "spark.sql.adaptive.enabled=false" \
    --conf "spark.jars.ivy=/tmp/.ivy2" \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,com.datastax.spark:spark-cassandra-connector_2.12:3.5.0 \
    /opt/spark/apps/spark_stream.py
    ```
6. Stop the entire pipeline:
    ```bash
    ./stop-pipeline.sh
    ```

# Monitoring and Management
- Kafka Control Center: [http://localhost:9021](http://localhost:9021)
- Airflow Web UI: [http://localhost:8081](http://localhost:8081) (username and password are configured in `.env` file, default is admin/admin)
- Spark Web UI: [http://localhost:9090](http://localhost:9090)

# âœ‰ï¸ Contact
Feel free to connect with me on the following platforms:
- Email: khanhnhan012@gmail.com
- [![Facebook](https://img.shields.io/badge/Facebook-1877F2?style=for-the-badge&logo=facebook&logoColor=white)](https://www.facebook.com/nguyen.khanh.nhan.905779)
- [![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/nhan-nguyen-b22023260/)